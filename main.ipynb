{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1_ax368.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ14amWvh5Ex"
      },
      "source": [
        "**LAB 1**\n",
        "\n",
        "**SPAM CLASSIFICATION**\n",
        "\n",
        "**PART 1:**\n",
        "SPAM Filtering using Naive Bayes Classifier.\n",
        "\n",
        "\n",
        "Importing all the relevant libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFTU_PiZJpuJ"
      },
      "source": [
        "import string \n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json\n",
        "from math import exp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfeFeCEJubeF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be41a0e5-e250-4de0-ac9d-c367dad30276"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7z4DlHSh8ul"
      },
      "source": [
        "The code below creates the dictionary of words by iterating through all the emails in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jek1Sd2KJuRk"
      },
      "source": [
        "def create_word_dictionary(train_dir):\n",
        "  d = dict() \n",
        "  no_of_mails = 0\n",
        "  emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)] \n",
        "  for mail in emails:\n",
        "      no_of_mails = no_of_mails + 1; \n",
        "      text = open(mail, \"r\")  \n",
        "      for line in text: \n",
        "          line = line.strip() \n",
        "          line = line.lower() \n",
        "          line = line.translate(line.maketrans(\"\", \"\", string.punctuation)) \n",
        "          words = line.split(\" \") \n",
        "          for word in words:\n",
        "              if (len(word) > 2) or (word !=''): \n",
        "                  d[word] = 0\n",
        "  d.pop('',None)\n",
        "  return d, no_of_mails  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MJxINbHiHBa"
      },
      "source": [
        "The code below is used to count the number of emails we are taking into consideration for training and test cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o7WnCJM-H9e"
      },
      "source": [
        "def count_emails(dir):\n",
        "  count = 0\n",
        "  emails = [os.path.join(train_dir,f) for f in os.listdir(dir)]\n",
        "  for mail in emails:\n",
        "    count = count+1\n",
        "  return count "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qta0gatHiOe9"
      },
      "source": [
        "The function create_feature_vector_TF creates the term frequency feature vector by iteratng over all the emails and then counting how many times a word occurs in the mail and then creating an array of these term frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWP-bGMvJwPe"
      },
      "source": [
        "def create_feature_vector_TF(train_dir, no_of_mails,d): \n",
        "  matrix = np.empty(shape=(no_of_mails,2),dtype='object')\n",
        "  i=0\n",
        "  emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
        "  for mail in emails:\n",
        "      text = open(mail, \"r\") \n",
        "      de = dict(d)\n",
        "      dictlist =[]\n",
        "      for line in text: \n",
        "          line = line.strip() \n",
        "          line = line.lower() \n",
        "          line = line.translate(line.maketrans(\"\", \"\", string.punctuation)) \n",
        "          words = line.split(\" \") \n",
        "          for word in words: \n",
        "              if (word in de) and (word != ''):\n",
        "                  de[word] = de[word]+1\n",
        "              else:\n",
        "                  if word !='':\n",
        "                      de[word] = 0\n",
        "      for key in de.keys():\n",
        "          temp = de[key]\n",
        "          dictlist.append(temp)        \n",
        "      if 'spm' in mail:\n",
        "          matrix[i][0] = dictlist\n",
        "          matrix[i][1] = 1\n",
        "      else:\n",
        "          matrix[i][0] = dictlist\n",
        "          matrix[i][1] = 0\n",
        "      i=i+1\n",
        "  X_train = np.empty(shape =(np.shape(matrix)[0],len(matrix[0][0])), dtype = int)\n",
        "  Y_train = np.empty(shape = (np.shape(matrix)[0],1))\n",
        "  for l in range(0, np.shape(matrix)[0]):\n",
        "    for k in range(0, len(matrix[0][0])):\n",
        "        X_train[l][k] = (matrix[l][0])[k] \n",
        "    Y_train[l] = matrix[l][1]\n",
        "  return X_train, Y_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_m8qBZNiipe"
      },
      "source": [
        "The function create_feature_vector_BF creates the feature vector by iteratng over all the emails and then checking if a word is present in the email or not and then creating an array of this information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5ktEIvHJzCI"
      },
      "source": [
        "def create_feature_vector_BF(train_dir, no_of_mails,d): \n",
        "  matrix = np.empty(shape=(no_of_mails,2),dtype='object')\n",
        "  i=0\n",
        "  emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
        "  for mail in emails:\n",
        "      text = open(mail, \"r\") \n",
        "      de = dict(d)\n",
        "      dictlist =[]\n",
        "      for line in text: \n",
        "          line = line.strip() \n",
        "          line = line.lower() \n",
        "          line = line.translate(line.maketrans(\"\", \"\", string.punctuation)) \n",
        "          words = line.split(\" \") \n",
        "          for word in words: \n",
        "              if (word in de) and (word != ''):\n",
        "                  de[word] = 1\n",
        "              else:\n",
        "                  if word !='':\n",
        "                      de[word] = 0\n",
        "      for key in de.keys():\n",
        "          temp = de[key]\n",
        "          dictlist.append(temp)        \n",
        "      if 'spm' in mail:\n",
        "          matrix[i][0] = dictlist\n",
        "          matrix[i][1] = 1\n",
        "      else:\n",
        "          matrix[i][0] = dictlist\n",
        "          matrix[i][1] = 0\n",
        "      i=i+1\n",
        "  X_train = np.empty(shape =(np.shape(matrix)[0],len(matrix[0][0])), dtype = int)\n",
        "  Y_train = np.empty(shape = (np.shape(matrix)[0],1))\n",
        "  for l in range(0, np.shape(matrix)[0]):\n",
        "    for k in range(0, len(matrix[0][0])):\n",
        "        X_train[l][k] = (matrix[l][0])[k] \n",
        "    Y_train[l] = matrix[l][1]\n",
        "  return X_train, Y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHhb6cdMixbm"
      },
      "source": [
        "The function below calculates the information gain for the features in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SImaIgjBJ3m0"
      },
      "source": [
        "def calc_information_gain(X_train, Y_train):\n",
        "  ig = mutual_info_classif(X_train,Y_train.ravel())\n",
        "  id_sort = np.argsort(ig, axis=-1)\n",
        "  return ig, id_sort\n",
        "# print(ig)\n",
        "# print(np.shape(X_train))\n",
        "# print(np.shape(ig))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C94zantJi5sM"
      },
      "source": [
        "Based on the Information Gain value, the code below selects N number of features with the highest IG value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPezAJg479mQ"
      },
      "source": [
        "def select_features(ig, id_sort, N, X_train):\n",
        "  FS_X_train = np.empty(shape = (np.shape(X_train)[0], N), dtype = int)\n",
        "\n",
        "  for p in range(0, np.shape(X_train)[0]):\n",
        "      z=0\n",
        "      for s in id_sort[-N:]:\n",
        "          FS_X_train[p][z] = X_train[p][s]\n",
        "          z = z+1\n",
        "  return FS_X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8FsdmSLjIDC"
      },
      "source": [
        "The code that is commented out below is caling the create_dictionary function and then writing the dictionary to a json file so that we don't have to create a dictionary everytime we run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8u5WpVQjEEu"
      },
      "source": [
        "train_dir = '/content/drive/My Drive/train'\n",
        "test_dir = '/content/drive/My Drive/test'\n",
        "\n",
        "# print(\"Creating dictionary of words...\")\n",
        "# d, no_of_train_mails = create_word_dictionary(train_dir)\n",
        "# print(\"Dictionary creation complete.\")\n",
        "# with open('/content/drive/My Drive/my_dict.json', 'w') as f:\n",
        "#     json.dump(d, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWVEKJ3NjTYv"
      },
      "source": [
        "The code below loads the json file(which was created in the previous step) as the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jefF_hH9PXg-"
      },
      "source": [
        "with open('/content/drive/My Drive/my_dict.json') as f:\n",
        "    d = json.load(f)\n",
        "\n",
        "no_of_train_mails = count_emails(train_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TXbKnQFjgFu"
      },
      "source": [
        "The code below creates the training set and test set for both binary features and term frequency features.\n",
        "This codes takes a lot of time(around 15-20 mins) to execute because the function that calculates the information gain is very intensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0XaHAGzKGMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "40779729-63e3-46cb-db08-ccfe1ef9e6a5"
      },
      "source": [
        "#Binary Features\n",
        "print(\"Now, creating Binary training feature vector...\")\n",
        "X_train_BF, Y_train_BF = create_feature_vector_BF(train_dir, no_of_train_mails, d)\n",
        "print(\"Training feature vector creation complete.\")\n",
        "no_of_test_mails = count_emails(test_dir)\n",
        "print(\"Now, creating test feature vector...\")\n",
        "X_test_BF, Y_test_BF = create_feature_vector_BF(test_dir, no_of_test_mails, d)\n",
        "print(\"Test feature vector creation complete.\")\n",
        "\n",
        "print(\"Calculating IG values...\")\n",
        "ig_BF,id_sort_BF = calc_information_gain(X_train_BF, Y_train_BF)\n",
        "print(\"IG calculation complete.\")\n",
        "\n",
        "#Term Frequency Features\n",
        "print(\"Now, creating Term Frequency training feature vector...\")\n",
        "X_train_TF, Y_train_TF = create_feature_vector_TF(train_dir, no_of_train_mails, d)\n",
        "print(\"Training feature vector creation complete.\")\n",
        "no_of_test_mails = count_emails(test_dir)\n",
        "print(\"Now, creating test feature vector...\")\n",
        "X_test_TF, Y_test_TF = create_feature_vector_BF(test_dir, no_of_test_mails, d)\n",
        "print(\"Test feature vector creation complete.\")\n",
        "\n",
        "print(\"Calculating TF IG values...\")\n",
        "ig_TF,id_sort_TF = calc_information_gain(X_train_TF, Y_train_TF)\n",
        "print(\"IG calculation complete.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now, creating Binary training feature vector...\n",
            "Training feature vector creation complete.\n",
            "Now, creating test feature vector...\n",
            "Test feature vector creation complete.\n",
            "Calculating IG values...\n",
            "IG calculation complete.\n",
            "Now, creating Term Frequency training feature vector...\n",
            "Training feature vector creation complete.\n",
            "Now, creating test feature vector...\n",
            "Test feature vector creation complete.\n",
            "Calculating TF IG values...\n",
            "IG calculation complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRZWD5QOj9HU"
      },
      "source": [
        "We are fitting our Naive Bayes Model to the data that we created above. The list N is the list of features to be used for training and testing the ML algorithm. pr_list is a list of precision and recall values for all the different classifiers and different feature sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTUE6Lj3rnME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "3ddc5d58-78e4-4b83-aeda-c97cdea369f0"
      },
      "source": [
        "N=[10, 100, 1000]\n",
        "clf1 = BernoulliNB()\n",
        "clf2 = MultinomialNB()\n",
        "pr_list = []\n",
        "\n",
        "for n in N:\n",
        "  pr = []\n",
        "  FS_X_train = select_features(ig_BF, id_sort_BF, n, X_train_BF)\n",
        "  FS_X_test = select_features(ig_BF, id_sort_BF, n, X_test_BF)\n",
        "\n",
        "  clf1.fit(FS_X_train, Y_train_BF.ravel())\n",
        "  Yhat1 = clf1.predict(FS_X_test)\n",
        "  print(\"Accuracy: \", clf1.score(FS_X_test, Y_test_BF.ravel()))\n",
        "  precision, recall, _, _ = precision_recall_fscore_support(Y_test_BF, Yhat1)\n",
        "  pr.append(precision[1])\n",
        "  pr.append(recall[1])\n",
        "  pr_list.append(pr)\n",
        "\n",
        "for n in N:\n",
        "  pr = []\n",
        "  FS_X_train = select_features(ig_BF, id_sort_BF, n, X_train_BF)\n",
        "  FS_X_test = select_features(ig_BF, id_sort_BF, n, X_test_BF)\n",
        "\n",
        "  clf2.fit(FS_X_train, Y_train_BF.ravel())\n",
        "  Yhat2 = clf2.predict(FS_X_test)\n",
        "  print(\"Accuracy: \", clf2.score(FS_X_test, Y_test_BF.ravel()))\n",
        "  precision, recall, _, _ = precision_recall_fscore_support(Y_test_BF, Yhat2)\n",
        "  pr.append(precision[1])\n",
        "  pr.append(recall[1])\n",
        "  pr_list.append(pr)\n",
        "\n",
        "for n in N:\n",
        "  pr = []\n",
        "  FS_X_train_TF = select_features(ig_TF, id_sort_TF, n, X_train_TF)\n",
        "  FS_X_test_TF = select_features(ig_TF, id_sort_TF, n, X_test_TF)\n",
        "\n",
        "  clf2.fit(FS_X_train_TF, Y_train_TF.ravel())\n",
        "  Yhat3 = clf2.predict(FS_X_test_TF)\n",
        "  print(\"Accuracy:\", clf2.score(FS_X_test_TF, Y_test_TF.ravel()))\n",
        "  precision, recall, _, _ = precision_recall_fscore_support(Y_test_TF, Yhat3)\n",
        "  pr.append(precision[1])\n",
        "  pr.append(recall[1])\n",
        "  pr_list.append(pr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9621993127147767\n",
            "Accuracy:  0.9553264604810997\n",
            "Accuracy:  0.9621993127147767\n",
            "Accuracy:  0.9518900343642611\n",
            "Accuracy:  0.9828178694158075\n",
            "Accuracy:  0.993127147766323\n",
            "Accuracy: 0.9553264604810997\n",
            "Accuracy: 0.9828178694158075\n",
            "Accuracy: 0.9896907216494846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkiOD2v7KG7o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "aef64345-8593-4246-f127-f20bdda0dda5"
      },
      "source": [
        "for i in range(0,len(pr_list)):\n",
        "  print(pr_list[i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8958333333333334, 0.8775510204081632]\n",
            "[1.0, 0.7346938775510204]\n",
            "[1.0, 0.7755102040816326]\n",
            "[0.926829268292683, 0.7755102040816326]\n",
            "[0.9782608695652174, 0.9183673469387755]\n",
            "[1.0, 0.9591836734693877]\n",
            "[0.8333333333333334, 0.9183673469387755]\n",
            "[0.9583333333333334, 0.9387755102040817]\n",
            "[1.0, 0.9387755102040817]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8nsmG2Gk6nT"
      },
      "source": [
        "The code below prints the list of top-10 words with the highest IG value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbP6Z_pdiz4m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dce6f91c-0cf1-45df-e56e-9f849efd09c1"
      },
      "source": [
        "top10_BF=[]\n",
        "top10_TF=[]\n",
        "words=list(d.keys())\n",
        "N=10\n",
        "for s in id_sort_BF[-N:]:\n",
        "        top10_BF.append(words[s])\n",
        "print(top10_BF)\n",
        "\n",
        "for s in id_sort_TF[-N:]:\n",
        "        top10_TF.append(words[s])\n",
        "print(top10_TF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['save', 'english', 'money', 'our', 'click', 'linguistic', 'university', 'free', 'remove', 'language']\n",
            "['market', 'click', 'business', 'our', 'money', 'university', 'linguistic', 'free', 'remove', 'language']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN_tyKIIlBkp"
      },
      "source": [
        "**Part 2.**\n",
        "\n",
        "**SVM Classifier**\n",
        "\n",
        "The code below is fitting our training set to an SVM classifier. To find the optimal parameters of the SVM Classifier, I am using 10-fold cross validation. The features are **Binary Features** because they provide a higher accuracy score than term frequency features. The Features are selected based on the highest **IG value**. So, the first 100 features with highest IG values are selected. This is because **IG value** provides a good estimate of how much information the presence and absence of each words contributes to the email being a spam or ham. I chose the 100 terms for the feature set because that provides a balance between accuracy and computation time.\n",
        "\n",
        "The GridSearchCV function is a very useful function provided by the sci-kit learn library that fits multiple models on a cross validation set. Multiple model refers to the same classifier with different parameters. The parameter array is a list of parameters that the GriSearchCV function uses to create the multiple models and then returns the parameter that give the best performance. \n",
        "\n",
        "The accuracy of the classifier with both TF and BF features can be seen below.\n",
        "\n",
        "We can see that the SVM model performs best when we use the radial basis function kernel and the regularization parameter is set to 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFJgUPBg_C0h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d923fffa-760e-4044-f483-4ee43c80dfc5"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "FS_X_train_BF = select_features(ig_BF, id_sort_BF, 100, X_train_BF)\n",
        "FS_X_test_BF = select_features(ig_BF, id_sort_BF, 100, X_test_BF)\n",
        "\n",
        "FS_X_train_TF = select_features(ig_TF, id_sort_TF, 100, X_train_TF)\n",
        "FS_X_test_TF = select_features(ig_TF, id_sort_TF, 100, X_test_TF)\n",
        "\n",
        "\n",
        "parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[1,5,10], 'degree':[1,2,3]}\n",
        "svc = SVC()\n",
        "clf1 = GridSearchCV(svc, parameters, cv=10)\n",
        "clf1.fit(FS_X_train_TF, Y_train_TF.ravel())\n",
        "Yhat_svm_TF = clf1.predict(FS_X_test_TF)\n",
        "print(clf1.best_estimator_)\n",
        "print(\"Accuracy: \", clf1.score(FS_X_test_TF, Y_test_TF.ravel()))\n",
        "precision_TF, recall_TF, _, _ = precision_recall_fscore_support(Y_test_TF, Yhat_svm_TF)\n",
        "print(\"Precision: \", precision_TF)\n",
        "print(\"Recall: \", recall_TF)\n",
        "\n",
        "clf2 = GridSearchCV(svc, parameters, cv=10)\n",
        "clf2.fit(FS_X_train_BF, Y_train_BF.ravel())\n",
        "Yhat_svm_BF = clf2.predict(FS_X_test_BF)\n",
        "print(clf2.best_estimator_)\n",
        "print(\"Accuracy: \", clf2.score(FS_X_test_BF, Y_test_BF.ravel()))\n",
        "precision_BF, recall_BF, _, _ = precision_recall_fscore_support(Y_test_BF, Yhat_svm_BF)\n",
        "print(\"Precision: \", precision_BF)\n",
        "print(\"Recall: \", recall_BF)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC(C=5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=1, gamma='scale', kernel='linear',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "Accuracy:  0.979381443298969\n",
            "Precision:  [0.98360656 0.95744681]\n",
            "Recall:  [0.99173554 0.91836735]\n",
            "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=1, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "Accuracy:  0.9828178694158075\n",
            "Precision:  [0.98367347 0.97826087]\n",
            "Recall:  [0.99586777 0.91836735]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjF9xx_Lqmdq"
      },
      "source": [
        "**Adversarial Classification:**\n",
        "**Attack and Defense** \n",
        "\n",
        "The code below take the top 10 fetures with highest IG value and uses it to train a Multinomial NB classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxqhzp-PIuJv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18159379-b892-4cf5-b359-976f622a6783"
      },
      "source": [
        "clf2 = MultinomialNB()\n",
        "n=10\n",
        "FS_X_train = select_features(ig_BF, id_sort_BF, n, X_train_BF)\n",
        "FS_X_test = select_features(ig_BF, id_sort_BF, n, X_test_BF)\n",
        "\n",
        "clf2.fit(FS_X_train, Y_train_BF.ravel())\n",
        "Yhat2 = clf2.predict(FS_X_test)\n",
        "print(\"Accuracy before modification: \", accuracy_score(Y_test_BF, Yhat2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy before modification:  0.9518900343642611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UCDptEnz4lJ"
      },
      "source": [
        "def perf_measure(y_actual, y_hat):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "\n",
        "    for i in range(len(y_hat)): \n",
        "        if y_actual[i]==y_hat[i]==1:\n",
        "           TP += 1\n",
        "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
        "           FP += 1\n",
        "        if y_actual[i]==y_hat[i]==0:\n",
        "           TN += 1\n",
        "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "           FN += 1\n",
        "\n",
        "    return(TP, FP, TN, FN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRhViriUz6N8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bc6e244-c64b-4a0b-90bc-2ee0caf52cac"
      },
      "source": [
        "TP,FP,TN,FN = perf_measure(Y_test_BF, Yhat2)\n",
        "\n",
        "print (\"False Negative Rate before attacker's modification: \", FN/(FN+TP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False Negative Rate before attacker's modification:  0.22448979591836735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA3tVfRSrByE"
      },
      "source": [
        "The code below calculates the minimum cost that the attacker/adversary has to pay so that the classifier classifies all the spam email as legit emails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q82TD0X9qbmk"
      },
      "source": [
        "def mcc(abc, cost, p):\n",
        "  abcd = abc\n",
        "  abc = abcd + sorted_Loc[0][p]\n",
        "  cost = cost+1;\n",
        "  if abc>=0:\n",
        "    cost = mcc(abc, cost, p+1)\n",
        "  return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3NRR6ahre9A"
      },
      "source": [
        "We are only considering 10 words in our machine learning model. This, according to me, is a limitation. This is because the words with highest IG value are the words that consist of info that distinguishes a spam email from a ham email. It could be a spam word or a ham word. The log odds of a ham word in the top 10 words would be negative and that of a spam word would be positive. According to the log odds calculation, only three words give information about the email being a ham. Since we only have 10 words and we can, at max, only add 3 of them to make a email get classified as spam instead of ham. If the email has a very high Loc value then adding the three words would not make the email classify it as legit. Adding other words will not have any effect because the classifier is only using these 10 words to determine if the email is spam or ham."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzK4AlLpqKNJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c69fadd8-10ae-45d0-9baf-7d61f57c73c7"
      },
      "source": [
        "p_xi_spam =np.zeros((1,10))\n",
        "p_xi_legit = np.zeros((1,10))\n",
        "for k in range(0,len(clf2.feature_log_prob_[0])):\n",
        "  p_xi_legit[0][k] = exp(clf2.feature_log_prob_[0][k])\n",
        "\n",
        "# print(p_xi_spam)\n",
        "for k in range(0,len(clf2.feature_log_prob_[0])):\n",
        "  p_xi_spam[0][k] = exp(clf2.feature_log_prob_[1][k])\n",
        "\n",
        "Loc = np.log((p_xi_spam/p_xi_legit))\n",
        "\n",
        "# print(p_xi_legit)\n",
        "Lox = np.zeros(np.shape(FS_X_test))\n",
        "i=0\n",
        "for l in range(0, np.shape(FS_X_test)[0]):\n",
        "  Lox[l] = (np.multiply(np.log((p_xi_spam/p_xi_legit)), FS_X_test[l]))\n",
        "\n",
        "\n",
        "delta = np.zeros(np.shape(FS_X_test))\n",
        "for i in range(0, np.shape(FS_X_test)[0]):\n",
        "  for j in range(0, 10):\n",
        "    delta[i][j] = max((Lox[i][j] - (1-Lox[i][j])),0)\n",
        "\n",
        "# print(delta)\n",
        "\n",
        "sorted_Loc = np.sort(Loc)\n",
        "idx = np.argsort(Loc)\n",
        "lis =[sorted_Loc, idx]\n",
        "\n",
        "\n",
        "cost=[]\n",
        "change_list = []\n",
        "for t in range(0, np.shape(Yhat2)[0]):\n",
        "    if Yhat2[t]==1:\n",
        "      if np.sum(Lox[t])<13.5:\n",
        "        change_list.append(t)\n",
        "        cost_each = mcc(np.sum(Lox[t]),0, 0)\n",
        "        cost.append(cost_each)\n",
        "\n",
        "print(\"Cost for each spam email to make it ham is: \",cost)\n",
        "print(\"Average cost of attacker is: \", np.average(cost))\n",
        "\n",
        "for w in range(0, np.shape(Yhat2)[0]):\n",
        "  if Yhat2[w] ==1:\n",
        "    for z in cost:\n",
        "      for y in range(0,z):\n",
        "        FS_X_test[w][idx[0][y]] = 1;\n",
        "\n",
        "Yhat_ad = clf2.predict(FS_X_test)\n",
        "print(\"Accuracy after adversary's attack: \", accuracy_score(Y_test_BF, Yhat_ad))\n",
        "\n",
        "TP,FP,TN,FN = perf_measure(Y_test_BF, Yhat_ad)\n",
        "\n",
        "print (\"False Negative Rate after attacker's modification: \", FN/(FN+TP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost for each spam email to make it ham is:  [2, 3, 1, 2, 3, 3, 1, 1, 1, 2, 1, 1, 4, 3, 2, 2, 1, 1, 2, 3, 2, 2, 3, 2, 4, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 4, 1, 2, 3]\n",
            "Average cost of attacker is:  1.9268292682926829\n",
            "Accuracy after adversary's attack:  0.8316151202749141\n",
            "False Negative Rate after attacker's modification:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeNOfeftsyXh"
      },
      "source": [
        "Below is the code that the defender modifies to protect against the adversary's modification. However, the accuracy is very low because we are only considering 10 words. Lowering the threshold will also have an effect on legit emails being classified as spam. This is why we get low accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWHwfwoqqLQC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "106edfd2-4b6f-4786-a66a-fc9dfaf9929e"
      },
      "source": [
        "New_Sum_Lox = np.zeros((np.shape(FS_X_test)[0],1))\n",
        "for l in range(0, np.shape(FS_X_test)[0]):\n",
        "  New_Sum_Lox[l] = np.sum(np.multiply((np.log(p_xi_spam/p_xi_legit)), FS_X_test[l]))\n",
        "\n",
        "change_mail = np.zeros((len(change_list),1))\n",
        "k=0\n",
        "for i in change_list:\n",
        "  change_mail[k] = New_Sum_Lox[i]\n",
        "  k+=1\n",
        "\n",
        "New_threshold = np.min(change_mail)\n",
        "\n",
        "yhat_thresh_changed = np.zeros(np.shape(Yhat2))\n",
        "\n",
        "for l in range(0, np.shape(X_test_BF)[0]):\n",
        "  if New_Sum_Lox[l]>New_threshold:\n",
        "    yhat_thresh_changed[l] = 1\n",
        "  else:\n",
        "    yhat_thresh_changed[l] = 0\n",
        "\n",
        "TP,FP,TN,FN = perf_measure(Y_test_BF, yhat_thresh_changed)\n",
        "\n",
        "print(\"False Positive Rate of updated classifier: \", FP/(FP+TP))\n",
        "print(\"False Negative Rate of updated classifier: \", FN/(FN+TP))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False Positive Rate of updated classifier:  0.8203125\n",
            "False Negative Rate of updated classifier:  0.061224489795918366\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
